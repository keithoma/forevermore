\section{Conclusion and Potential Experiments}\label{cha:conclusion}

In the realm of computing, no matter the choice of the increment the approximation will always be just that, \textit{an approximation}. Uniform convergence happens only up to a point, and beyond that point decreasing the increment is fruitless, counterproductive even. Furthermore, we have seen that this sweet spot as we called it, depends on how strongly the input affects the output of the function, i.e. if the change in output is small, then a larger increment produces a relativly good result.

As we hypothesized earlier in section \ref{cha:error}, we believe that the exact spot of the sweet spot depends on the machine precision used. Even more, our conjecture is that if
\begin{align*}
    h <& \frac{\epsilon}{h} \hspace{0.5cm} \text{for the approximation of the first derivative, and}\\
    h <& \frac{\epsilon}{h^2} \hspace{0.5cm} \text{for the approximation of the second derivative,}
\end{align*}
then the approximation becomes worse. This hypothesis coincide with our results of the protocol, but to confirm this, we would need further experiments.