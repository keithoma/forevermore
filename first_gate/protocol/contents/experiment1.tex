\section{The Sweet Spot of the Increment}
To test our theory, we consider the following function
\begin{align*}
    g_1(x) := \frac{\sin{x}}{x} \text{,}
\end{align*}
and its derivatives
\begin{align*}
    {g'}_{1}(x) = \frac{x \cdot \cos{x} - \sin{x}}{x^2} \hspace{1cm} {g''}_{1}(x) = \frac{(x^2 - 2) \sin{x} + 2 x \cos{x}}{x^3} \text{}
\end{align*}
on the interval \(I := [\pi, 3\pi]\). Note that we are bounded by the limitation of a computer, i.e. the interval \(I\) is partitioned into \(p := 1000\) many grid points and only on these points the functions will be evaluated.


\subsection{Approximation's Approach} % dont like the tile

We first want to vizualize the functions we are working with. Therefore, using the Python module, we have drawn the plot of \(g_1\), its first two exact derivatives, \( (D^{(1)}_h g_1) \) and \( (D^{(2)}_h g_1 )\). For the increment \(h\) we have chosen the following values
\begin{align*}
    \frac{\pi}{3} \text{,} \hspace{0.5cm} \frac{\pi}{4} \text{,} \hspace{0.5cm} \frac{\pi}{5} \text{,} \hspace{0.5cm} \frac{\pi}{10} \text{.}
\end{align*}
See figure \ref{fig:exp1_h} for the resulting graph.

%%% figure the 4 plots for the h
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{graphics/plot_h/exp1_huge_h.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{graphics/plot_h/exp1_large_h.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{graphics/plot_h/exp1_small_h.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{graphics/plot_h/exp1_tiny_h.png}
    \end{subfigure}
    \caption{Plots of the Function and its Derivatives with Various Increments}
    \label{fig:exp1_h}
\end{figure}

As one can clearly see, both \((D^{(1)}_h g_1)\) and \((D^{(2)}_hg_2)\) merges to the analytic derivatives of \(g_1\) as \(h\) becomes small. \((D^{(2)}_h g_1)\) is almost indistinguishable from \(g_1''\) for \(h = \frac{\pi}{10}\). This result confirms the first part of the theory -- the approximation becomes better as the increment tends to \(0\). That \((D^{(2)}_h g_2)\) converges faster to \(g_1''\) is also expected because the rate of convergence of the approximation of the second derivative is faster than the one for the first. But admittingly, even \(h = \frac{\pi}{10}\) is rather large if one compares it to the 64 bit precision offered by \texttt{float64}. Can we indefinitely improve our approximation if we just choose \(h\) to be small enough? Unfortunately, it turns out that this is not the case.

\subsection{The Anatomy of Errors}\label{cha:error}

Now, we consider the two errors \(e_{g_1}^{(1)}(h)\) and \(e_{g_1}^{(2)}(h)\) which are again, the largest disparity between the analytic and the approximation on the interval \(I\). We have drawn the plot (see \ref{fig:exp1_error_plot}) in double log scale for the error for
\[h \in [10^{-9}, 10^2] \text{.} \]

%%% figure of the error plot
\begin{figure}[h!]
    \includegraphics[width=\linewidth]{graphics/error_plot/exp1_error_plot.png}
    \caption{The Error Plot}
    \label{fig:exp1_error_plot}
\end{figure}

Before we discuss the bad news, let us examine where the theory was right in its prediction. The middle part of \(e_f^{(1)}(h)\) and \(e_f^{(2)}(h)\) are parallel to the lines of \(h\) and \(h^2\) respectively. This is as we hypothesized since the order of convergence of the error are
\[\mathcal{O}(h) \hspace{0.3cm} \text{for} \hspace{0.3cm} e_f^{(1)}(h) \hspace{0.3cm} \text{and} \hspace{0.3cm} \mathcal{O}(h^2) \hspace{0.3cm} \text{for} \hspace{0.3cm} e_f^{(2)}(h) \text{.}\]
The left side of the plot, however, does not behave the way we wanted. At around \(10^{-8}\) for \(e_f^{(1)}(h)\) and at \(10^{-4}\) for \(e_f^{(2)}(h)\) both plots stop converging and starts increasing again. This means that on the machine number line the approximation does not uniformly converge to the analytic derivative. Instead, there is an optimal point for the increment, a "sweet spot", so to speak. Beyond this sweet spot, the approximation actually becomes worse and thus, we cannot decrease the value of the increment haphazardly in hopes of getting a better approximation.

But why does this happen? Where exactly in our computation does the imperfectness of the machine sneak in? The answer is hidden in the way we compute the approximations. We had
\begin{align*}
    D^{(1)}_h(x) &= \frac{f(x + h) - f(x)}{h} \\
    D^{(2)}_h(x) &= \frac{f(x + h) - 2 f(x)}{h^2} + f(x - h) \text{.}
\end{align*}
% here maybe we could takl about the conditioning of f?
For \(D^{(1)}_h\), if \(h\) becomes small enough, then \(g_1(x + h) \approx g_1(x)\) since \(g_1\) is continuous. Hence at the numerator, we are essentially subtracting two numbers which are close to each other, but such subtractions are ill-conditioned \cite{Lecture}. In other words, for \(h\) which are small enough, we loose precision at this point of the approximation process resulting in the error graph we have seen at \ref{fig:exp1_error_plot}. Similar argumentation also works for the second derivative as \(- f(x + h) + 2 f(x) \approx f(x - h)\) for small \(h\).

It is important to keep in mind that the threshhold for \(h\) being \textit{small enough} depends most likely on the machine precision used for the computation. Our result is based on a 64-Bit float and while in general, the approximation will never converge to the analytic derivative outside of our mathematical imagination, the exact spot where the error stops decreasing might differ. We will discuss this hypothesis in section \ref{cha:error}.