\section{The Sweet Spot of the Increment}
To test our theory, we consider the following function
\begin{align*}
    g_1(x) := \frac{\sin{x}}{x} \text{,}
\end{align*}
and its derivatives
\begin{align*}
    {g'}_{1}(x) = \frac{x \cdot \cos{x} - \sin{x}}{x^2} \hspace{1cm} {g''}_{1}(x) = \frac{(x^2 - 2) \sin{x} + 2 x \cos{x}}{x^3} \text{}
\end{align*}
on the interval \(I := [\pi, 3\pi]\). Note that we are bounded by the limitation of a computer, i.e. the interval \(I\) is partitioned into \(p := 1000\) many grid points and only on these points the functions will be evaluated.


\subsection{Approaching the Approximation} % dont like the tile

We first want to get a picture of what we are working with. Therefore, using the Python module, we have drawn the plot of \(f\), its first two exact derivatives, \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\). For the increment \(h\) we have chosen the following values
\begin{align*}
    \frac{\pi}{3} \text{,} \hspace{0.5cm} \frac{\pi}{4} \text{,} \hspace{0.5cm} \frac{\pi}{5} \text{,} \hspace{0.5cm} \frac{\pi}{10} \text{.}
\end{align*}
See \ref{XXX} for the resulting graph. \\

%%% figure the 4 plots for the h

As one can clearly see, both \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\) merges to the analytic derivatives of \(f\) as \(h\) becomes smaller. \(D^{(2)}_h(x)\) is almost indistinguishable from \(f''\) for \(h = \frac{\pi}{10}\). This result confirms the first part of the theory -- the approximation becomes better as the increment tends to \(0\). That \(D^{(2)}_h(x)\) converges faster to \(f''\) is also expected from the theory. But admittingly, even \(h = \frac{\pi}{10}\) is rather large if one compares it to the 64-bit precision we have worked with and most modern computers have to offer. Can we indefinitely improve our approximation if we just choose \(h\) to be small enough? Unfortunately, it turns out that this is not the case.

\subsection{The Anatomy of Errors}

Now, we consider the two errors \(e_f^{(1)}(h)\) and \(e_f^{(2)}(h)\) which are again, the largest disparity between the analytic and the approximation on the interval \(I\). We have drawn the plot (see \ref{xxx}) in double log scale for the error for
\[h \in [10^{-9}, 10^2] \text{.} \]

%%% figure of the error plot

Before we discuss the bad news, let us examine where the theory was right in its prediction. The middle part of \(e_f^{(1)}(h)\) and \(e_f^{(2)}(h)\) are parallel to the lines of \(h\) and \(h^2\) respectively. This is as we hypothesized since the order of convergence of the error are
\[\mathcal{O}(h) \hspace{0.3cm} \text{for} \hspace{0.3cm} e_f^{(1)}(h) \hspace{0.3cm} \text{and} \hspace{0.3cm} \mathcal{O}(h^2) \hspace{0.3cm} \text{for} \hspace{0.3cm} e_f^{(2)}(h) \text{.}\]
The left side of the plot, however, does not behave the way we wanted. At around \(10^{-8}\) for \(e_f^{(1)}(h)\) and at \(10^{-4}\) for \(e_f^{(2)}(h)\) both plots stop converging and starts increasing again. This means that on the machine number line the approximation does not uniformly converge to the analytic derivative. Instead, there is an optimal point for the increment, a "sweet spot", so to speak. Beyond this sweet spot, the approximation actually becomes worse and thus, we cannot decrease the value of the increment haphazardly in hopes of getting a better approximation.

But why does this happen? Where exactly in our computation does the imperfectness of the machine sneak in? To answer this question, see figure \ref{xxx}

%%% figure

Here, we have fixed \(x = \pi + 2 \approx 5.14\) to observe the behavior of the analytic derivatives and the approximations with \(h\) as the variable. At around \(10^{-8}\) and \(10^{-4}\) respectively, both \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\) starts oscillating even though in theory, these blue curves should once again smoothly approach the exact solution of \(f'\) and \(f''\) at \(x = \pi + 2\). This oscillation results in the error not converging.

Therefore, we proceed by looking at the definition of the approximation. We had
\begin{align*}
    D^{(1)}_h(x) &:= \frac{f(x + h) - f(x)}{h} \\
    D^{(2)}_h(x) &:= \frac{f(x + h) + f(x - h)- 2 f(x)}{h^2} \text{.}
\end{align*}
% here maybe we could takl about the conditioning of f?
For \(D^{(1)}_h\), if \(h\) becomes small enough, then \(g_1(x + h) \approx g_1(x)\) since \(g_1\) is continuous. Hence at the numerator, we are essentially subtracting two numbers which are close to each other, but such subtractions are ill-conditioned (see \cite{Lecture}). In other words, for \(h\) which are small enough, we loose accuracy at this point of the approximation process resulting in the error graph we have seen at \ref{XXX}. Similar argumentation also works for the second derivative as \(f(x + h) + f(x - h) \approx 2 f(x)\) for small \(h\).

It is important to keep in mind that the threshhold for \(h\) being \textit{small enough} depends on the machine precision used for the computation. Our result is based on a 64-Bit float and while in general, the approximation will never converge to the analytic derivative outside of our mathematical imagination, the exact spot where the error stops decreasing might differ.