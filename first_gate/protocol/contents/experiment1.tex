\section{The Sweet Spot of the Increment}
To test our theory, we consider the following function
\begin{align*}
    g_1(x) := \frac{\sin{x}}{x} \text{,}
\end{align*}
and its derivatives
\begin{align*}
    {g'}_{1}(x) = \frac{x \cdot \cos{x} - \sin{x}}{x^2} \hspace{1cm} {g''}_{1}(x) = \frac{(x^2 - 2) \sin{x} + 2 x \cos{x}}{x^3} \text{}
\end{align*}
on the interval \(I := [\pi, 3\pi]\). Note that we are bounded by the limitation of a computer, i.e. the interval \(I\) is partitioned into \(p := 1000\) many grid points and only on these points the functions will be evaluated.


\subsection{Plotting the Functions} % dont like the tile

We first want to get a picture of what we are working with. Therefore, using the Python module, we have drawn the plot of \(f\), its first two exact derivatives, \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\). For the increment \(h\) we have chosen the following values
\begin{align*}
    \frac{\pi}{3} \text{,} \hspace{0.5cm} \frac{\pi}{4} \text{,} \hspace{0.5cm} \frac{\pi}{5} \text{,} \hspace{0.5cm} \frac{\pi}{10} \text{.}
\end{align*}
See \ref{XXX} for the resulting graph. \\

%%% figure the 4 plots for the h

As one can clearly see, both \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\) merges to the analytic derivatives of \(f\) as \(h\) becomes smaller. \(D^{(2)}_h(x)\) is almost indistinguishable from \(f''\) for \(h = \frac{\pi}{10}\). This result confirms the first part of the theory -- the approximation becomes better as the increment tends to \(0\). That \(D^{(2)}_h(x)\) converges faster to \(f''\) is also expected from the theory. But admittingly, even \(h = \frac{\pi}{10}\) is rather large if one compares it to the 64-bit precision we have worked with and most modern computers have to offer. Can we indefinitely improve our approximation if we just choose \(h\) to be small enough? Unfortunately, it turns out that this is not the case.

\subsection{The Anatomy of Errors}

Now, we consider the two errors \(e_f^{(1)}(h)\) and \(e_f^{(2)}(h)\) which are again, the largest disparity between the analytic and the approximation on the interval \(I\). We have drawn the plot (see \ref{xxx}) in double log scale for the error for
\[h \in [10^{-9}, 10^2] \text{.} \]

%%% figure of the error plot

Before we discuss the bad news, let us examin the part where the theory was right. The middle part of \(e_f^{(1)}(h)\) and \(e_f^{(2)}(h)\) are parallel to the lines of \(h\) and \(h^2\) respectively. This is as we hypothesized since the order of convergence of the error are
\[\mathcal{O}(h) \text{ for } e_f^{(1)}(h) \text{ and } \mathcal{O}(h^2) \text{ for } e_f^{(2)}(h) \text{.}\]
The left side of the plot, however, does not behave the way we wanted. At around \(10^{-8}\) for \(e_f^{(1)}(h)\) and at \(10^{-4}\) for \(e_f^{(2)}(h)\) both plots stop converging and starts increasing again. This means that on the machine number line the approximation does not uniformly converge to the analytic one creating an optimal point for the increment. Beyond this "sweet spot", the approximation actually becomes worse. Thus, we cannot decrease the value of the increment haphazardly in hopes of getting a better approximation.

But why does this happen? Where exactly in our computation does the imperfectness of the computer sneak in? To answer the question, see figure \ref{xxx}

%%% figure

Here, we have fixed \(x = \pi + 2 \approx 5.14\) to observe the behavior of the analytic derivatives and the approximations with \(h\) as the variable. At around \(10^{-8}\) and \(10^{-4}\) respectively, both \(D^{(1)}_h(x)\) and \(D^{(2)}_h(x)\) starts oscillating resulting in the 



\section{How the Variance in Input changes the Output}